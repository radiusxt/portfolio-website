---
title: "Optimisation Strategies for BERT"
publishedAt: "2025-11-10"
summary: "Research project on optimisation strategies and hyperparameter variations for BERT."
image: "/images/projects/bert/huggingface.svg"
team:
  - name: "Nathaniel Chan"
    linkedIn: "https://www.linkedin.com/in/nathaniel-chan-/"
link: "https://github.com/radiusxt/BERT-Optimisation"
---

# Overview

The topic of Natural Language Processing (NLP) is a highly investigated subfield of machine learning bordering between both statistics and linguistics. The training and fine-tuning of large language models (LLMs) remains a critical segment in achieving strong downstream language performance in NLPs. BERT (Bidirectional Encoder Representations from Transformers), is a natural language processing model designed to understand the context of a word by simultaneously analyzing the text that precedes and succeeds it to capture deeper meaning.

# Key Objectives

- Transformers (BERT)
  - Built a modular training suite for a wide variety of input parameters to support hyperparameter optimisation across datasets for sentiment analysis tasks.
  - Observed differences in early training performance and convergence rates with standard and noisy data.
- Documentation
  - Compiled a detailed research paper documenting dataset characteristics, algorithm architecture, model structure, results and evaluation suite.
  - Generated several graphs and figures to support raw findings with error margins.

# Tech Stack

- Python
  - Partitioning data, generating normal and noisy datasets and tokenising phrases.
  - Tuning BERT on a variety of optimisers and a range of hyperparameter settings.
  - Tracking training progress with tqdm.
- Jupyter
  - Producing results via cell-based execution for several experiemnts without reloading the full dataset each time.
- Pytorch
  - Running the training loop with a selected training set for several iterations.
  - Evaluating the best trained model with a testing set.
- AWS
  - Model training on large datasets is a compute intensive task. Initially, staging was done locally before training more extensively on a compute cluster.
- Anaconda
  - Dependency management and environment reproucability.
  - Avoids any version incompatibility throughout development.
- GitHub
  - Version control for managing project structure.

# Challenges and Insights

Tuning BERT for sentiment analysis revealed the model's ability to analyze text demands significant compute resources, ultimately necessitating the use of an AWS GPU instance to manage the quadratic memory complexity of the transformer architecture. This computational requirement was further exacerbated by the high-precision batching to prevent insufficient memory errors during the optimiser process. A technical insight gained was the model's high sensitivity to data integrity. The introduction of noise created significant performance drawdowns as it disrupted tokenization and misled the bidirectional attention mechanism.

# Outcomes

The overall performance on the fine tuned BERT model sustained 85% accuracy with the AdamW optimiser where it yielded high stability in early training without forgetting its pretrained knowledge. This research project enhanced my knowledge and practical experience in pytorch, transformers and model training using real GPU compute.

# References

- [HuggingFace](https://huggingface.co/docs/transformers/en/model_doc/bert)
- [Model](https://huggingface.co/google-bert/bert-base-uncased)
